{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for image classification with PyTorch\n",
    "\n",
    "Credits: \\\n",
    "https://jovian.ai/aakashns/03-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147383db22ed420ca05d2eb6b7115bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3770809b4a48daaa824833ba5db36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f869a9597541779962bcb698e0281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2adfa8ee8e45a1931c20c4e52e4659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/u1094325/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset. PyTorch datasets allow us to specify one or more transformation functions which are applied to the images as they are loaded. `torchvision.transforms` contains many such predefined functions, and we'll use the `ToTensor` transform to convert images into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now converted to a 1x28x28 tensor. The first dimension is used to keep track of the color channels. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in which case there are 3 channels: red, green and blue (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now created data loaders to help us load the data in batches. We'll use a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Since `nn.Linear` expects the each training example to be a vector, each `1x28x28` image tensor needs to be flattened out into a vector of size `784 (28*28)`, before being passed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0284,  0.0113,  0.0141,  ..., -0.0258, -0.0244, -0.0353],\n",
       "        [ 0.0054, -0.0173, -0.0292,  ...,  0.0069,  0.0329, -0.0138],\n",
       "        [ 0.0169, -0.0018, -0.0167,  ...,  0.0357,  0.0042,  0.0326],\n",
       "        ...,\n",
       "        [ 0.0021,  0.0237, -0.0037,  ...,  0.0347, -0.0014,  0.0194],\n",
       "        [-0.0017, -0.0218, -0.0063,  ...,  0.0033,  0.0294,  0.0020],\n",
       "        [ 0.0184,  0.0328, -0.0341,  ..., -0.0118, -0.0226,  0.0197]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0237, -0.0254, -0.0069, -0.0129, -0.0330, -0.0158, -0.0099, -0.0355,\n",
       "         0.0337,  0.0066], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our images are of the shape `1x28x28`, but we need them to be vectors of size 784 i.e. we need to flatten them out. We'll use the `.reshape` method of a tensor, which will allow us to efficiently 'view' each image as a flat vector, without really chaging the underlying data. To include this additional functionality within our model, we need to define a custom model, by extending the `nn.Module` class from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model no longer has `.weight` and `.bias` attributes (as they are now inside the `.linear` attribute), but it does have a `.parameters` method which returns a list containing the weights and bias, and can be used by a PyTorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0083, -0.0021, -0.0299,  ...,  0.0011,  0.0346,  0.0056],\n",
       "         [-0.0038,  0.0134, -0.0306,  ...,  0.0320, -0.0332,  0.0334],\n",
       "         [ 0.0159,  0.0130,  0.0026,  ..., -0.0233,  0.0233, -0.0231],\n",
       "         ...,\n",
       "         [ 0.0066, -0.0018,  0.0270,  ...,  0.0137, -0.0316,  0.0019],\n",
       "         [-0.0018, -0.0051,  0.0179,  ...,  0.0208,  0.0339,  0.0266],\n",
       "         [-0.0166, -0.0055,  0.0070,  ..., -0.0324, -0.0122,  0.0349]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0097,  0.0089, -0.0159,  0.0043,  0.0256,  0.0207, -0.0017, -0.0074,\n",
       "          0.0273,  0.0159], requires_grad=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[ 0.2555, -0.0367,  0.1905,  0.2261, -0.1945,  0.0038, -0.0754, -0.0099,\n",
      "          0.1901, -0.0313],\n",
      "        [ 0.1399,  0.2762,  0.2464,  0.0913, -0.1057, -0.0498, -0.1166,  0.0078,\n",
      "          0.2616, -0.1155]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is included in the `torch.nn.functional` package, and requires us to specify a dimension along which the softmax must be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.1213, 0.0906, 0.1137, 0.1178, 0.0774, 0.0943, 0.0871, 0.0930, 0.1136,\n",
      "         0.0911],\n",
      "        [0.1067, 0.1223, 0.1187, 0.1016, 0.0834, 0.0882, 0.0825, 0.0935, 0.1205,\n",
      "         0.0826]])\n",
      "Sum:  1.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 9, 6, 9, 8, 6, 8, 1, 8, 8, 2, 2, 8, 8, 6, 1, 0, 0, 8, 8, 3, 4, 6,\n",
      "        9, 8, 5, 8, 4, 2, 2, 8, 8, 2, 8, 8, 0, 7, 8, 2, 2, 2, 8, 7, 1, 8, 2, 6,\n",
      "        1, 8, 8, 9, 8, 9, 7, 8, 6, 8, 0, 0, 4, 1, 7, 8, 6, 8, 7, 8, 8, 9, 8, 8,\n",
      "        8, 8, 8, 9, 8, 6, 8, 8, 8, 1, 8, 1, 0, 8, 2, 0, 8, 1, 0, 6, 2, 0, 9, 4,\n",
      "        5, 8, 8, 8, 6, 2, 8, 6, 4, 2, 8, 8, 8, 7, 6, 7, 4, 8, 8, 8, 8, 8, 8, 3,\n",
      "        1, 7, 6, 2, 8, 5, 8, 1])\n",
      "tensor([0.1213, 0.1223, 0.1226, 0.1316, 0.1212, 0.1550, 0.1107, 0.1255, 0.1143,\n",
      "        0.1311, 0.1412, 0.1145, 0.1156, 0.1374, 0.1381, 0.1259, 0.1131, 0.1195,\n",
      "        0.1070, 0.1749, 0.1550, 0.1156, 0.1193, 0.1130, 0.1310, 0.1629, 0.1241,\n",
      "        0.1179, 0.1153, 0.1287, 0.1329, 0.1293, 0.1317, 0.1233, 0.1579, 0.1573,\n",
      "        0.1124, 0.1259, 0.1236, 0.1302, 0.1255, 0.1247, 0.1410, 0.1128, 0.1320,\n",
      "        0.1309, 0.1491, 0.1176, 0.1254, 0.1458, 0.1399, 0.1286, 0.1645, 0.1301,\n",
      "        0.1400, 0.1312, 0.1164, 0.1712, 0.1147, 0.1384, 0.1209, 0.1162, 0.1288,\n",
      "        0.1156, 0.1224, 0.1272, 0.1124, 0.1480, 0.1235, 0.1133, 0.1715, 0.1410,\n",
      "        0.1374, 0.1389, 0.1287, 0.1486, 0.1256, 0.1254, 0.1720, 0.1367, 0.1291,\n",
      "        0.1154, 0.1278, 0.1329, 0.1310, 0.1367, 0.1239, 0.1160, 0.1378, 0.1197,\n",
      "        0.1309, 0.1208, 0.1188, 0.1181, 0.1295, 0.1125, 0.1266, 0.1577, 0.1220,\n",
      "        0.1272, 0.1171, 0.1216, 0.1286, 0.1289, 0.1112, 0.1227, 0.1167, 0.1558,\n",
      "        0.1181, 0.1507, 0.1206, 0.1281, 0.1163, 0.1251, 0.1307, 0.1420, 0.1462,\n",
      "        0.1196, 0.1236, 0.1293, 0.1418, 0.1162, 0.1125, 0.1437, 0.1248, 0.1301,\n",
      "        0.1137, 0.1162], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 3, 1, 3, 0, 1, 3, 9, 9, 2, 7, 1, 2, 3, 1, 8, 9, 8, 6, 5, 7, 3, 9,\n",
       "        0, 0, 9, 7, 3, 1, 7, 9, 7, 7, 0, 6, 1, 7, 2, 5, 5, 7, 8, 6, 7, 9, 8, 1,\n",
       "        4, 8, 6, 0, 8, 4, 0, 5, 8, 0, 2, 6, 9, 9, 5, 3, 2, 8, 3, 9, 9, 3, 0, 0,\n",
       "        6, 4, 0, 3, 6, 1, 2, 2, 6, 9, 1, 9, 5, 3, 6, 1, 2, 8, 8, 7, 9, 6, 3, 3,\n",
       "        5, 6, 7, 5, 1, 9, 0, 1, 1, 1, 1, 4, 1, 3, 1, 3, 7, 6, 6, 0, 0, 4, 9, 5,\n",
       "        7, 2, 7, 8, 9, 2, 2, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0547)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3698, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.322314977645874, 'val_acc': 0.08287183195352554}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9539, val_acc: 0.6312\n",
      "Epoch [1], val_loss: 1.6822, val_acc: 0.7445\n",
      "Epoch [2], val_loss: 1.4785, val_acc: 0.7759\n",
      "Epoch [3], val_loss: 1.3244, val_acc: 0.7921\n",
      "Epoch [4], val_loss: 1.2059, val_acc: 0.8081\n"
     ]
    }
   ],
   "source": [
    "## Train for 5 epochs:\n",
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1129, val_acc: 0.8167\n",
      "Epoch [1], val_loss: 1.0384, val_acc: 0.8236\n",
      "Epoch [2], val_loss: 0.9774, val_acc: 0.8280\n",
      "Epoch [3], val_loss: 0.9267, val_acc: 0.8326\n",
      "Epoch [4], val_loss: 0.8840, val_acc: 0.8358\n"
     ]
    }
   ],
   "source": [
    "## 5 more epochs:\n",
    "history2 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8474, val_acc: 0.8391\n",
      "Epoch [1], val_loss: 0.8157, val_acc: 0.8410\n",
      "Epoch [2], val_loss: 0.7880, val_acc: 0.8435\n",
      "Epoch [3], val_loss: 0.7635, val_acc: 0.8451\n",
      "Epoch [4], val_loss: 0.7418, val_acc: 0.8472\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7222, val_acc: 0.8495\n",
      "Epoch [1], val_loss: 0.7046, val_acc: 0.8514\n",
      "Epoch [2], val_loss: 0.6886, val_acc: 0.8535\n",
      "Epoch [3], val_loss: 0.6740, val_acc: 0.8550\n",
      "Epoch [4], val_loss: 0.6607, val_acc: 0.8565\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs. No. of epochs')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`img.unsqueeze` simply adds another dimension at the begining of the `1x28x28` tensor, making it a `1x1x28x28` tensor, which the model views as a batch containing a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM4ElEQVR4nO3db6xU9Z3H8c9nWZoY6QNQce9alC7xgc3GgCIxQTfXkDYsPsBGuikPGjZpvH2Apo0NWeM+wIeN2bZZn5DcRlO6YW1IqEqMcSHYSBq18WJQLr0BkbBwyxVsMCmYGES/++AeN1ecc2acMzNn4Pt+JZOZOd85Z74Z7odz5vyZnyNCAK5+f9N0AwAGg7ADSRB2IAnCDiRB2IEk/naQb2abXf9An0WEW02vtWa3vdb2EdvHbD9WZ1kA+svdHme3PU/SUUnfljQt6U1JGyPiTxXzsGYH+qwfa/ZVko5FxPGIuCjpt5LW11gegD6qE/abJJ2a83y6mPYFtsdsT9ieqPFeAGqqs4Ou1abClzbTI2Jc0rjEZjzQpDpr9mlJS+Y8/4ak0/XaAdAvdcL+pqRbbX/T9tckfV/S7t60BaDXut6Mj4hLth+W9D+S5kl6JiIO96wzAD3V9aG3rt6M7+xA3/XlpBoAVw7CDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJdj88uSbZPSDov6VNJlyJiZS+aAtB7tcJeuC8i/tKD5QDoIzbjgSTqhj0k7bF9wPZYqxfYHrM9YXui5nsBqMER0f3M9t9HxGnbiyXtlfRIROyveH33bwagIxHhVtNrrdkj4nRxf1bSc5JW1VkegP7pOuy2r7X99c8fS/qOpMleNQagt+rsjb9R0nO2P1/Of0fEyz3pCkDP1frO/pXfjO/sQN/15Ts7gCsHYQeSIOxAEoQdSIKwA0n04kKYFDZs2FBae+ihhyrnPX36dGX9448/rqzv2LGjsv7++++X1o4dO1Y5L/JgzQ4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXDVW4eOHz9eWlu6dOngGmnh/PnzpbXDhw8PsJPhMj09XVp78sknK+edmLhyf0WNq96A5Ag7kARhB5Ig7EAShB1IgrADSRB2IAmuZ+9Q1TXrt99+e+W8U1NTlfXbbrutsn7HHXdU1kdHR0trd999d+W8p06dqqwvWbKksl7HpUuXKusffPBBZX1kZKTr9z558mRl/Uo+zl6GNTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH17FeBhQsXltaWL19eOe+BAwcq63fddVdXPXWi3e/lHz16tLLe7vyFRYsWldY2b95cOe+2bdsq68Os6+vZbT9j+6ztyTnTFtnea/vd4r78rw3AUOhkM/7XktZeNu0xSfsi4lZJ+4rnAIZY27BHxH5J5y6bvF7S9uLxdkkP9LgvAD3W7bnxN0bEjCRFxIztxWUvtD0maazL9wHQI32/ECYixiWNS+ygA5rU7aG3M7ZHJKm4P9u7lgD0Q7dh3y1pU/F4k6QXetMOgH5pe5zd9rOSRiVdL+mMpK2Snpe0U9LNkk5K+l5EXL4Tr9Wy2IxHxx588MHK+s6dOyvrk5OTpbX77ruvct5z59r+OQ+tsuPsbb+zR8TGktKaWh0BGChOlwWSIOxAEoQdSIKwA0kQdiAJLnFFYxYvLj3LWpJ06NChWvNv2LChtLZr167Kea9kDNkMJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZDMa0+7nnG+44YbK+ocfflhZP3LkyFfu6WrGmh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuB6dvTV6tWrS2uvvPJK5bzz58+vrI+OjlbW9+/fX1m/WnE9O5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kwfXs6Kt169aV1todR9+3b19l/fXXX++qp6zartltP2P7rO3JOdOesP1n2weLW/m/KICh0Mlm/K8lrW0x/ZcRsby4vdTbtgD0WtuwR8R+SecG0AuAPqqzg+5h2+8Um/kLy15ke8z2hO2JGu8FoKZuw75N0jJJyyXNSPp52QsjYjwiVkbEyi7fC0APdBX2iDgTEZ9GxGeSfiVpVW/bAtBrXYXd9sicp9+VNFn2WgDDoe1xdtvPShqVdL3taUlbJY3aXi4pJJ2Q9KM+9oghds0111TW165tdSBn1sWLFyvn3bp1a2X9k08+qazji9qGPSI2tpj8dB96AdBHnC4LJEHYgSQIO5AEYQeSIOxAElziilq2bNlSWV+xYkVp7eWXX66c97XXXuuqJ7TGmh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmDIZlS6//77K+vPP/98Zf2jjz4qrVVd/ipJb7zxRmUdrTFkM5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kwfXsyV133XWV9aeeeqqyPm/evMr6Sy+Vj/nJcfTBYs0OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwPftVrt1x8HbHuu+8887K+nvvvVdZr7pmvd286E7X17PbXmL797anbB+2/eNi+iLbe22/W9wv7HXTAHqnk834S5J+GhG3Sbpb0mbb35L0mKR9EXGrpH3FcwBDqm3YI2ImIt4qHp+XNCXpJknrJW0vXrZd0gP9ahJAfV/p3HjbSyWtkPRHSTdGxIw0+x+C7cUl84xJGqvXJoC6Og677QWSdkn6SUT81W65D+BLImJc0nixDHbQAQ3p6NCb7fmaDfqOiPhdMfmM7ZGiPiLpbH9aBNALbdfsnl2FPy1pKiJ+Mae0W9ImST8r7l/oS4eoZdmyZZX1dofW2nn00Ucr6xxeGx6dbMavlvQDSYdsHyymPa7ZkO+0/UNJJyV9rz8tAuiFtmGPiD9IKvuCvqa37QDoF06XBZIg7EAShB1IgrADSRB2IAl+SvoqcMstt5TW9uzZU2vZW7Zsqay/+OKLtZaPwWHNDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJz9KjA2Vv6rXzfffHOtZb/66quV9UH+FDnqYc0OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnP0KcM8991TWH3nkkQF1gisZa3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKKT8dmXSPqNpL+T9Jmk8Yj4T9tPSHpI0gfFSx+PiJf61Whm9957b2V9wYIFXS+73fjpFy5c6HrZGC6dnFRzSdJPI+It21+XdMD23qL2y4j4j/61B6BXOhmffUbSTPH4vO0pSTf1uzEAvfWVvrPbXipphaQ/FpMetv2O7WdsLyyZZ8z2hO2JWp0CqKXjsNteIGmXpJ9ExF8lbZO0TNJyza75f95qvogYj4iVEbGyB/0C6FJHYbc9X7NB3xERv5OkiDgTEZ9GxGeSfiVpVf/aBFBX27DbtqSnJU1FxC/mTB+Z87LvSprsfXsAeqWTvfGrJf1A0iHbB4tpj0vaaHu5pJB0QtKP+tIhann77bcr62vWrKmsnzt3rpftoEGd7I3/gyS3KHFMHbiCcAYdkARhB5Ig7EAShB1IgrADSRB2IAkPcshd24zvC/RZRLQ6VM6aHciCsANJEHYgCcIOJEHYgSQIO5AEYQeSGPSQzX+R9L9znl9fTBtGw9rbsPYl0Vu3etnbLWWFgZ5U86U3tyeG9bfphrW3Ye1LorduDao3NuOBJAg7kETTYR9v+P2rDGtvw9qXRG/dGkhvjX5nBzA4Ta/ZAQwIYQeSaCTsttfaPmL7mO3HmuihjO0Ttg/ZPtj0+HTFGHpnbU/OmbbI9l7b7xb3LcfYa6i3J2z/ufjsDtpe11BvS2z/3vaU7cO2f1xMb/Szq+hrIJ/bwL+z254n6aikb0ualvSmpI0R8aeBNlLC9glJKyOi8RMwbP+TpAuSfhMR/1hMe1LSuYj4WfEf5cKI+Lch6e0JSReaHsa7GK1oZO4w45IekPSvavCzq+jrXzSAz62JNfsqScci4nhEXJT0W0nrG+hj6EXEfkmXD8myXtL24vF2zf6xDFxJb0MhImYi4q3i8XlJnw8z3uhnV9HXQDQR9psknZrzfFrDNd57SNpj+4DtsaabaeHGiJiRZv94JC1uuJ/LtR3Ge5AuG2Z8aD67boY/r6uJsLf6faxhOv63OiLukPTPkjYXm6voTEfDeA9Ki2HGh0K3w5/X1UTYpyUtmfP8G5JON9BHSxFxurg/K+k5Dd9Q1Gc+H0G3uD/bcD//b5iG8W41zLiG4LNrcvjzJsL+pqRbbX/T9tckfV/S7gb6+BLb1xY7TmT7Wknf0fANRb1b0qbi8SZJLzTYyxcMyzDeZcOMq+HPrvHhzyNi4DdJ6zS7R/49Sf/eRA8lff2DpLeL2+Gme5P0rGY36z7R7BbRDyVdJ2mfpHeL+0VD1Nt/STok6R3NBmukod7u0exXw3ckHSxu65r+7Cr6GsjnxumyQBKcQQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfrLwRQMBWyxMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6388569474220276, 'val_acc': 0.861132800579071}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 1.9862e-02,  7.9617e-03,  1.8048e-02,  ...,  2.6265e-02,\n",
       "                        2.8510e-02, -1.5775e-02],\n",
       "                      [ 1.8872e-03,  3.5121e-02,  1.7515e-02,  ...,  1.0292e-02,\n",
       "                        2.8281e-02,  7.9140e-03],\n",
       "                      [-5.6099e-03,  6.9677e-04, -8.6186e-03,  ...,  3.0285e-03,\n",
       "                       -2.4729e-02,  9.1767e-04],\n",
       "                      ...,\n",
       "                      [-1.2073e-02,  9.3495e-03, -2.7398e-02,  ..., -2.0053e-02,\n",
       "                        2.7167e-02,  2.7191e-02],\n",
       "                      [ 8.0229e-03,  2.3412e-02,  2.5715e-02,  ..., -1.8835e-03,\n",
       "                        3.2801e-02, -7.0600e-03],\n",
       "                      [ 1.8568e-03, -1.4662e-02,  2.1652e-02,  ..., -4.3947e-05,\n",
       "                        3.3690e-02,  3.2620e-02]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0676,  0.1066, -0.0148, -0.0209,  0.0085,  0.0105, -0.0359,  0.0425,\n",
       "                      -0.1183, -0.0054]))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model weights, we can instante a new object of the class `MnistModel`, and use the `.load_state_dict method`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 1.9862e-02,  7.9617e-03,  1.8048e-02,  ...,  2.6265e-02,\n",
       "                        2.8510e-02, -1.5775e-02],\n",
       "                      [ 1.8872e-03,  3.5121e-02,  1.7515e-02,  ...,  1.0292e-02,\n",
       "                        2.8281e-02,  7.9140e-03],\n",
       "                      [-5.6099e-03,  6.9677e-04, -8.6186e-03,  ...,  3.0285e-03,\n",
       "                       -2.4729e-02,  9.1767e-04],\n",
       "                      ...,\n",
       "                      [-1.2073e-02,  9.3495e-03, -2.7398e-02,  ..., -2.0053e-02,\n",
       "                        2.7167e-02,  2.7191e-02],\n",
       "                      [ 8.0229e-03,  2.3412e-02,  2.5715e-02,  ..., -1.8835e-03,\n",
       "                        3.2801e-02, -7.0600e-03],\n",
       "                      [ 1.8568e-03, -1.4662e-02,  2.1652e-02,  ..., -4.3947e-05,\n",
       "                        3.3690e-02,  3.2620e-02]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0676,  0.1066, -0.0148, -0.0209,  0.0085,  0.0105, -0.0359,  0.0425,\n",
       "                      -0.1183, -0.0054]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MnistModel()\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6388569474220276, 'val_acc': 0.861132800579071}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model2, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
